{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ijsbqc7Ipl2"
   },
   "source": [
    "## Full DPWGAN rewrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QifCHxjTI5Ql"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from models import dp_wgan, pate_gan, ron_gauss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KcDKmo_aisiA"
   },
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "MODEL_NAME = 'DPWGAN' # Don't change this\n",
    "DATASET_NAME = 'churn' # Choose either 'churn' or 'marketing' exactly!\n",
    "TARGET_VARIABLE = 'Exited' # either 'Exited' or 'Response'\n",
    "TRAIN_TEST_RATIO = 0.25\n",
    "LEAKY = True # Put false for normal relu.\n",
    "\n",
    "# These seem to be good/important to tune from what I can tell from the github.\n",
    "TARGET_EPSILON = 10\n",
    "TARGET_DELTA = 1e-4\n",
    "SIGMA = 0.8\n",
    "NUM_EPOCHS = 500\n",
    "LEARNING_RATE = 5e-5\n",
    "\n",
    "# Following defaults in the toolbox. Might not be crucial to tune these\n",
    "MICRO_BATCH_SIZE = 8\n",
    "BATCH_SIZE = 64\n",
    "ENABLE_PRIVACY = True\n",
    "CLIP_COEFF = 0.1\n",
    "CLAMP_LOWER = -0.01\n",
    "CLAMP_UPPER = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6LXKHydGhhEF"
   },
   "outputs": [],
   "source": [
    "# Read in data and do train test split\n",
    "df = pd.read_csv(f'{DATASET_NAME}_processed.csv')\n",
    "df_train, df_test = train_test_split(df, test_size=TRAIN_TEST_RATIO, random_state=42, stratify = df[TARGET_VARIABLE])\n",
    "\n",
    "# Initialise logfile path\n",
    "timestamp = int(time.time())\n",
    "logfile = f'log_{DATASET_NAME}_{MODEL_NAME}_{timestamp}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0rmYtc28ZXk4"
   },
   "outputs": [],
   "source": [
    "# Grab x and y from the respective dataframes and convert to numpy arrays.\n",
    "train_x = df_train.drop(columns=TARGET_VARIABLE).values\n",
    "train_y = df_train[TARGET_VARIABLE].values\n",
    "test_x = df_test.drop(columns=TARGET_VARIABLE).values\n",
    "test_y = df_test[TARGET_VARIABLE].values\n",
    "\n",
    "# Initialise scaler and use this to normalize the inputs.\n",
    "scaler = StandardScaler()\n",
    "train_x = scaler.fit_transform(train_x)\n",
    "test_x = scaler.transform(test_x)\n",
    "\n",
    "# Some misc variables for pategan \n",
    "data_columns = [col for col in df_train.columns if col != TARGET_VARIABLE]\n",
    "class_ratios = df_train[TARGET_VARIABLE].sort_values().groupby(df_train[TARGET_VARIABLE]).size().values/train_x.shape[0]\n",
    "input_dim = train_x.shape[1]\n",
    "z_dim = int(input_dim / 4 + 1) if input_dim % 4 == 0 else int(input_dim / 4)\n",
    "conditional = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vMGXxmhnmEpz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 Loss D real :  0.010714411980568423 Loss D fake :  0.010565648695409038 Loss G :  0.010633213267995894 Epsilon spent :  2.623153807427913\n",
      "Epoch : 2 Loss D real :  0.010862018328220055 Loss D fake :  0.010656586445916558 Loss G :  0.010703517456732443 Epsilon spent :  2.9437225218617797\n",
      "Epoch : 3 Loss D real :  0.010819494412316784 Loss D fake :  0.010821979859933582 Loss G :  0.010839053784986325 Epsilon spent :  3.224422132116374\n",
      "Epoch : 4 Loss D real :  0.011106140410721886 Loss D fake :  0.010958945494663145 Loss G :  0.01094224207200507 Epsilon spent :  3.339819054074313\n",
      "Epoch : 5 Loss D real :  0.011274419941166205 Loss D fake :  0.011028902119101746 Loss G :  0.011236928058126532 Epsilon spent :  3.455215976032252\n",
      "Epoch : 6 Loss D real :  0.010773818496483938 Loss D fake :  0.01116729468878628 Loss G :  0.011298002508992348 Epsilon spent :  3.5706128979901908\n",
      "Epoch : 7 Loss D real :  0.011116367533215982 Loss D fake :  0.01120728272566263 Loss G :  0.011383343818824759 Epsilon spent :  3.6860098199481297\n",
      "Epoch : 8 Loss D real :  0.010847272369035404 Loss D fake :  0.011271753820337614 Loss G :  0.011476646596827109 Epsilon spent :  3.8014067419060686\n",
      "Epoch : 9 Loss D real :  0.011337085039748297 Loss D fake :  0.011433051325611598 Loss G :  0.011335183613087874 Epsilon spent :  3.916803663864008\n",
      "Epoch : 10 Loss D real :  0.01091926861275543 Loss D fake :  0.011591341985790104 Loss G :  0.011720347048154217 Epsilon spent :  4.032200585821947\n",
      "Epoch : 11 Loss D real :  0.011069429109791338 Loss D fake :  0.011579358739409922 Loss G :  0.011774136480234324 Epsilon spent :  4.147597507779886\n",
      "Epoch : 12 Loss D real :  0.010884184235941669 Loss D fake :  0.011625134650223691 Loss G :  0.011711551136392396 Epsilon spent :  4.262994429737825\n",
      "Epoch : 13 Loss D real :  0.011753703571535493 Loss D fake :  0.011720559523970338 Loss G :  0.011656717914785111 Epsilon spent :  4.378391351695764\n",
      "Epoch : 14 Loss D real :  0.010836821477930378 Loss D fake :  0.011929286207360182 Loss G :  0.011482471053837703 Epsilon spent :  4.493788273653703\n",
      "Epoch : 15 Loss D real :  0.010643105722024752 Loss D fake :  0.011635502545138593 Loss G :  0.011786833940701145 Epsilon spent :  4.609185195611642\n",
      "Epoch : 16 Loss D real :  0.011143489774142609 Loss D fake :  0.011648570436452527 Loss G :  0.011683788099423793 Epsilon spent :  4.724582117569581\n",
      "Epoch : 17 Loss D real :  0.011060907204775877 Loss D fake :  0.011978117376297741 Loss G :  0.011531288787906305 Epsilon spent :  4.8399790395275195\n",
      "Epoch : 18 Loss D real :  0.010609544255347799 Loss D fake :  0.01168324174196527 Loss G :  0.011842463240566464 Epsilon spent :  4.9553759614854584\n",
      "Epoch : 19 Loss D real :  0.011540119010254944 Loss D fake :  0.011573394760251265 Loss G :  0.011641856894534282 Epsilon spent :  5.070772883443397\n",
      "Epoch : 20 Loss D real :  0.011083365145304811 Loss D fake :  0.011614012874392466 Loss G :  0.011686372111255807 Epsilon spent :  5.186169805401336\n",
      "Epoch : 21 Loss D real :  0.01090720533023436 Loss D fake :  0.01167898036176183 Loss G :  0.011686986683286556 Epsilon spent :  5.301566727359275\n",
      "Epoch : 22 Loss D real :  0.011137016961184092 Loss D fake :  0.011556195533109766 Loss G :  0.011367149613465567 Epsilon spent :  5.416963649317214\n",
      "Epoch : 23 Loss D real :  0.010936089268592188 Loss D fake :  0.011498833434453798 Loss G :  0.011670091997591607 Epsilon spent :  5.532360571275153\n",
      "Epoch : 24 Loss D real :  0.010890433046230481 Loss D fake :  0.011412836284465745 Loss G :  0.011520346082449066 Epsilon spent :  5.647757493233092\n",
      "Epoch : 25 Loss D real :  0.011413231540000554 Loss D fake :  0.011499707923363911 Loss G :  0.011606995418536388 Epsilon spent :  5.750413762592017\n",
      "Epoch : 26 Loss D real :  0.011050414716850716 Loss D fake :  0.01144867068955763 Loss G :  0.011529528603420873 Epsilon spent :  5.826924640229428\n",
      "Epoch : 27 Loss D real :  0.011028831047825498 Loss D fake :  0.0114325084545238 Loss G :  0.011503218348021326 Epsilon spent :  5.903435517866839\n",
      "Epoch : 28 Loss D real :  0.011227485121483633 Loss D fake :  0.01145293923324952 Loss G :  0.01142363246585961 Epsilon spent :  5.97994639550425\n",
      "Epoch : 29 Loss D real :  0.011880500205578848 Loss D fake :  0.011345123631119223 Loss G :  0.011421593448033741 Epsilon spent :  6.05645727314166\n",
      "Epoch : 30 Loss D real :  0.010783003891265332 Loss D fake :  0.011263286336345586 Loss G :  0.01146340673483501 Epsilon spent :  6.132968150779072\n",
      "Epoch : 31 Loss D real :  0.011202819646271654 Loss D fake :  0.01131900417999317 Loss G :  0.011597643009183718 Epsilon spent :  6.2094790284164825\n",
      "Epoch : 32 Loss D real :  0.011318533604300369 Loss D fake :  0.011556326275665451 Loss G :  0.011543782408973741 Epsilon spent :  6.285989906053894\n",
      "Epoch : 33 Loss D real :  0.01095036270553521 Loss D fake :  0.01159403261263944 Loss G :  0.011341658474156194 Epsilon spent :  6.362500783691305\n",
      "Epoch : 34 Loss D real :  0.011306762898109729 Loss D fake :  0.011435125632502392 Loss G :  0.011297156474176798 Epsilon spent :  6.439011661328715\n",
      "Epoch : 35 Loss D real :  0.010532672950180106 Loss D fake :  0.01128331505007709 Loss G :  0.011454047126337148 Epsilon spent :  6.515522538966127\n",
      "Epoch : 36 Loss D real :  0.011216814471909725 Loss D fake :  0.011104989741085751 Loss G :  0.011401038321084323 Epsilon spent :  6.592033416603538\n",
      "Epoch : 37 Loss D real :  0.011654581715432464 Loss D fake :  0.011148246990180768 Loss G :  0.01138385815262843 Epsilon spent :  6.668544294240949\n",
      "Epoch : 38 Loss D real :  0.01174075009022916 Loss D fake :  0.01118941710537177 Loss G :  0.01140467544517422 Epsilon spent :  6.745055171878359\n",
      "Epoch : 39 Loss D real :  0.011222293409450062 Loss D fake :  0.011382997515937167 Loss G :  0.011784385338328984 Epsilon spent :  6.82156604951577\n",
      "Epoch : 40 Loss D real :  0.01122669036079019 Loss D fake :  0.01141041030013617 Loss G :  0.011344935004405608 Epsilon spent :  6.898076927153181\n",
      "Epoch : 41 Loss D real :  0.010561918941599836 Loss D fake :  0.011275837568877067 Loss G :  0.011347903522600017 Epsilon spent :  6.974587804790593\n",
      "Epoch : 42 Loss D real :  0.011089877830633704 Loss D fake :  0.011487781504544197 Loss G :  0.011280379112273463 Epsilon spent :  7.0510986824280035\n",
      "Epoch : 43 Loss D real :  0.011552119533163497 Loss D fake :  0.011280024800821473 Loss G :  0.011145764115988964 Epsilon spent :  7.127609560065414\n",
      "Epoch : 44 Loss D real :  0.010570025235830513 Loss D fake :  0.011221515737723663 Loss G :  0.011287849512422198 Epsilon spent :  7.204120437702825\n",
      "Epoch : 45 Loss D real :  0.010947949344983414 Loss D fake :  0.011145083825018148 Loss G :  0.011166187084184143 Epsilon spent :  7.280631315340236\n",
      "Epoch : 46 Loss D real :  0.01129179224469224 Loss D fake :  0.011158950477390848 Loss G :  0.011306273558599544 Epsilon spent :  7.357142192977648\n",
      "Epoch : 47 Loss D real :  0.011151951929130502 Loss D fake :  0.011153360664984544 Loss G :  0.011322684706869864 Epsilon spent :  7.433653070615058\n",
      "Epoch : 48 Loss D real :  0.010970889514657026 Loss D fake :  0.011150264876383986 Loss G :  0.011074441947092757 Epsilon spent :  7.510163948252469\n",
      "Epoch : 49 Loss D real :  0.011444378405946967 Loss D fake :  0.011256796703643766 Loss G :  0.011207402005869532 Epsilon spent :  7.5866748258898795\n",
      "Epoch : 50 Loss D real :  0.011126225707767922 Loss D fake :  0.011028340930342044 Loss G :  0.011107756548771244 Epsilon spent :  7.663185703527291\n",
      "Epoch : 51 Loss D real :  0.011075908338570325 Loss D fake :  0.010982699089759245 Loss G :  0.011052518870682138 Epsilon spent :  7.7396965811647025\n",
      "Epoch : 52 Loss D real :  0.011015494766230746 Loss D fake :  0.011273333389282263 Loss G :  0.011178351425528883 Epsilon spent :  7.816207458802113\n",
      "Epoch : 53 Loss D real :  0.011283145999666754 Loss D fake :  0.011388680575504491 Loss G :  0.011152238972721145 Epsilon spent :  7.892718336439524\n",
      "Epoch : 54 Loss D real :  0.01114351373010285 Loss D fake :  0.01103213467290452 Loss G :  0.011055487670763996 Epsilon spent :  7.969229214076934\n",
      "Epoch : 55 Loss D real :  0.010822571115821468 Loss D fake :  0.011106338667067214 Loss G :  0.010909325620101894 Epsilon spent :  8.045740091714347\n",
      "Epoch : 56 Loss D real :  0.011135395858852969 Loss D fake :  0.010970673669962757 Loss G :  0.01107662708311898 Epsilon spent :  8.122250969351757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 57 Loss D real :  0.01072805911413499 Loss D fake :  0.011086269023043313 Loss G :  0.011157023167082508 Epsilon spent :  8.198761846989168\n",
      "Epoch : 58 Loss D real :  0.010861468766195313 Loss D fake :  0.010934986540215224 Loss G :  0.010910634755485114 Epsilon spent :  8.275272724626578\n",
      "Epoch : 59 Loss D real :  0.011016693464063046 Loss D fake :  0.011084017991827346 Loss G :  0.011005661293023687 Epsilon spent :  8.351783602263989\n",
      "Epoch : 60 Loss D real :  0.011113357908168359 Loss D fake :  0.010942385972109175 Loss G :  0.011067815049257104 Epsilon spent :  8.428294479901401\n",
      "Epoch : 61 Loss D real :  0.01124603302902224 Loss D fake :  0.010866413861424225 Loss G :  0.010969408958357688 Epsilon spent :  8.504805357538812\n",
      "Epoch : 62 Loss D real :  0.011186023165965913 Loss D fake :  0.011065262554048653 Loss G :  0.011053722421445693 Epsilon spent :  8.581316235176223\n",
      "Epoch : 63 Loss D real :  0.010766347148978391 Loss D fake :  0.011084791617975596 Loss G :  0.011198287882256536 Epsilon spent :  8.657827112813633\n",
      "Epoch : 64 Loss D real :  0.011151030055277344 Loss D fake :  0.011020821427622327 Loss G :  0.010988539508694242 Epsilon spent :  8.734337990451044\n",
      "Epoch : 65 Loss D real :  0.011151437507734854 Loss D fake :  0.011019826317621023 Loss G :  0.011034072852644834 Epsilon spent :  8.810848868088456\n",
      "Epoch : 66 Loss D real :  0.010994357001354855 Loss D fake :  0.010703369256311268 Loss G :  0.01080286033143849 Epsilon spent :  8.887359745725867\n",
      "Epoch : 67 Loss D real :  0.011343670225924368 Loss D fake :  0.011106323750152108 Loss G :  0.0109621482307981 Epsilon spent :  8.963870623363277\n",
      "Epoch : 68 Loss D real :  0.010843720096680954 Loss D fake :  0.01070563333986167 Loss G :  0.010834573724316458 Epsilon spent :  9.040381501000688\n",
      "Epoch : 69 Loss D real :  0.010737879070577685 Loss D fake :  0.010904673115756564 Loss G :  0.010912948046158857 Epsilon spent :  9.116892378638099\n",
      "Epoch : 70 Loss D real :  0.01108264327621762 Loss D fake :  0.010883236613185548 Loss G :  0.010948789465411632 Epsilon spent :  9.193403256275511\n",
      "Epoch : 71 Loss D real :  0.01099494353655399 Loss D fake :  0.010895500301294509 Loss G :  0.011021682652440839 Epsilon spent :  9.269914133912922\n",
      "Epoch : 72 Loss D real :  0.011293505865665592 Loss D fake :  0.011025340670597171 Loss G :  0.010649198501337269 Epsilon spent :  9.346425011550332\n",
      "Epoch : 73 Loss D real :  0.011277640850880196 Loss D fake :  0.010933483581168819 Loss G :  0.010810766953203465 Epsilon spent :  9.422935889187743\n",
      "Epoch : 74 Loss D real :  0.01065970494647615 Loss D fake :  0.010953152689178128 Loss G :  0.010881535064378677 Epsilon spent :  9.499446766825153\n",
      "Epoch : 75 Loss D real :  0.010534234607290562 Loss D fake :  0.010961031156658592 Loss G :  0.011030240957822788 Epsilon spent :  9.575957644462566\n",
      "Epoch : 76 Loss D real :  0.010587803391816879 Loss D fake :  0.01095966018847622 Loss G :  0.010931363968569346 Epsilon spent :  9.652468522099976\n",
      "Epoch : 77 Loss D real :  0.010761155522656477 Loss D fake :  0.010897510888540008 Loss G :  0.010910301012337222 Epsilon spent :  9.728979399737387\n",
      "Epoch : 78 Loss D real :  0.010998280944878265 Loss D fake :  0.010838650620273818 Loss G :  0.010762810154760355 Epsilon spent :  9.805490277374798\n",
      "Epoch : 79 Loss D real :  0.010878650878514485 Loss D fake :  0.010887420005813176 Loss G :  0.0108847135754884 Epsilon spent :  9.872865544445002\n",
      "Epoch : 80 Loss D real :  0.01077090719378529 Loss D fake :  0.010764573159772473 Loss G :  0.01097273260962623 Epsilon spent :  9.924971909153355\n",
      "Epoch : 81 Loss D real :  0.010962561201747302 Loss D fake :  0.010737110533245706 Loss G :  0.010860779124222415 Epsilon spent :  9.977078273861707\n",
      "Epoch : 82 Loss D real :  0.010804270039623826 Loss D fake :  0.01075402128341788 Loss G :  0.010903465973434617 Epsilon spent :  10.029184638570062\n"
     ]
    }
   ],
   "source": [
    "# Initialise hyperparams and do TRAINING\n",
    "Hyperparams = collections.namedtuple(\n",
    "        'Hyperarams',\n",
    "        'batch_size micro_batch_size clamp_lower clamp_upper clip_coeff sigma class_ratios lr num_epochs')\n",
    "Hyperparams.__new__.__defaults__ = (None, None, None, None, None, None, None, None, None)\n",
    "\n",
    "model = dp_wgan.DP_WGAN(LEAKY, logfile, input_dim, z_dim, TARGET_EPSILON, TARGET_DELTA, conditional)\n",
    "model.train(train_x, train_y, Hyperparams(batch_size=BATCH_SIZE, micro_batch_size=MICRO_BATCH_SIZE,\n",
    "                                              clamp_lower=CLAMP_LOWER, clamp_upper=CLAMP_UPPER,\n",
    "                                              clip_coeff=CLIP_COEFF, sigma=SIGMA, class_ratios=class_ratios, lr=LEARNING_RATE, \n",
    "                                              num_epochs=NUM_EPOCHS), private=ENABLE_PRIVACY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate synthetic data using trained model, then save in CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FoR8Lqbmhace"
   },
   "outputs": [],
   "source": [
    "# Helper functions for saving the synthetic data...\n",
    "def update_array(indexes):\n",
    "    b = np.zeros((indexes.size, indexes.max()+1))\n",
    "    b[np.arange(indexes.size), indexes] = 1\n",
    "    return b\n",
    "\n",
    "def save_marketing():\n",
    "    # Some fancy indexing to get the actual synthetic data..\n",
    "    accepted = np.argmax(syn_save[:,16:21], axis=1)\n",
    "    education = np.argmax(syn_save[:, 22:27], axis=1)\n",
    "    marital = np.argmax(syn_save[:, 27:34], axis=1)\n",
    "    country = np.argmax(syn_save[:, 34:], axis=1)\n",
    "\n",
    "    syn_save[:,16:21] = update_array(accepted)\n",
    "    syn_save[:, 22:27] = update_array(education)\n",
    "    syn_save[:, 27:34] = update_array(marital)\n",
    "    syn_save[:, 34:] = update_array(country)\n",
    "\n",
    "    df1 = pd.DataFrame(syn_save, columns = df.columns.drop(TARGET_VARIABLE))\n",
    "    df2 = pd.DataFrame(syn_y, columns = [TARGET_VARIABLE])\n",
    "    df_save = pd.concat([df1,df2], axis =1)\n",
    "    df_save.to_csv(f'synthetic_{MODEL_NAME}_{DATASET_NAME}.csv')\n",
    "\n",
    "def save_churn():\n",
    "    geography = np.argmax(syn_save[:,8:11], axis=1)\n",
    "    gender = np.argmax(syn_save[:,11:], axis=1)\n",
    "    \n",
    "    syn_save[:,8:11] = update_array(geography)\n",
    "    syn_save[:, 11:] = update_array(gender)\n",
    "    \n",
    "    df1 = pd.DataFrame(syn_save, columns = df.columns.drop(TARGET_VARIABLE))\n",
    "    df2 = pd.DataFrame(syn_y, columns = [TARGET_VARIABLE])\n",
    "    df_save = pd.concat([df1,df2], axis =1)\n",
    "    df_save.to_csv(f'synthetic_{MODEL_NAME}_{DATASET_NAME}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to generate data and save them.\n",
    "syn_data = model.generate(train_x.shape[0], class_ratios)\n",
    "syn_x, syn_y = syn_data[:, :-1], syn_data[:, -1]\n",
    "\n",
    "# Make a copy for saving\n",
    "syn_save = scaler.inverse_transform(syn_x)\n",
    "\n",
    "# Save data to csv using functions\n",
    "if DATASET_NAME == 'churn': save_churn()\n",
    "else: save_marketing()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do classification using Neural Networks and look at ROC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2346,
     "status": "ok",
     "timestamp": 1622894670836,
     "user": {
      "displayName": "Tee Yee Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgKFrQtw-OyvubwOe5xvtv9JOTRkaAVybEHvmq9ag=s64",
      "userId": "07106232567566268752"
     },
     "user_tz": -480
    },
    "id": "QRtEB0Orm7ah",
    "outputId": "fcd7a205-093c-4447-c924-0cd4cd27e463"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC Score 0.5491247943841591\n",
      "Accuracy 0.4612\n"
     ]
    }
   ],
   "source": [
    "# Train using Synthetic data, using simple neural network.\n",
    "mlp = MLPClassifier((32,8), max_iter=1000, random_state=42)\n",
    "mlp.fit(syn_x, syn_y)\n",
    "pred_y = mlp.predict(test_x)\n",
    "\n",
    "print('ROC Score', roc_auc_score(test_y, pred_y))\n",
    "print('Accuracy', mlp.score(test_x,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3867,
     "status": "ok",
     "timestamp": 1622894688257,
     "user": {
      "displayName": "Tee Yee Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgKFrQtw-OyvubwOe5xvtv9JOTRkaAVybEHvmq9ag=s64",
      "userId": "07106232567566268752"
     },
     "user_tz": -480
    },
    "id": "nB_uyefotvDF",
    "outputId": "d4bb6ab0-3b38-4031-ba65-4d29183f4c7a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC Score 0.7328967583990432\n",
      "Accuracy 0.8564\n"
     ]
    }
   ],
   "source": [
    "# Train using REAL data, using simple neural network.\n",
    "mlp = MLPClassifier((32,8), max_iter=1000, random_state=42)\n",
    "mlp.fit(train_x, train_y)\n",
    "pred_y = mlp.predict(test_x)\n",
    "\n",
    "print('ROC Score', roc_auc_score(test_y, pred_y))\n",
    "print('Accuracy', mlp.score(test_x,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dpwgan-fullscript.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:pategan]",
   "language": "python",
   "name": "conda-env-pategan-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
