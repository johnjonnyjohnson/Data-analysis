{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ijsbqc7Ipl2"
   },
   "source": [
    "## Full DPWGAN rewrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QifCHxjTI5Ql"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from models import dp_wgan, pate_gan, ron_gauss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KcDKmo_aisiA"
   },
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "MODEL_NAME = 'DPWGAN' # Don't change this\n",
    "DATASET_NAME = 'churn' # Choose either 'churn' or 'marketing' exactly!\n",
    "TARGET_VARIABLE = 'Exited' # either 'Exited' or 'Response'\n",
    "TRAIN_TEST_RATIO = 0.25\n",
    "LEAKY = True # Put false for normal relu.\n",
    "\n",
    "# These seem to be good/important to tune from what I can tell from the github.\n",
    "TARGET_EPSILON = 10\n",
    "TARGET_DELTA = 1e-4\n",
    "SIGMA = 0.8\n",
    "NUM_EPOCHS = 500\n",
    "LEARNING_RATE = 5e-5\n",
    "\n",
    "# Following defaults in the toolbox. Might not be crucial to tune these\n",
    "MICRO_BATCH_SIZE = 8\n",
    "BATCH_SIZE = 64\n",
    "ENABLE_PRIVACY = True\n",
    "CLIP_COEFF = 0.1\n",
    "CLAMP_LOWER = -0.01\n",
    "CLAMP_UPPER = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6LXKHydGhhEF"
   },
   "outputs": [],
   "source": [
    "# Read in data and do train test split\n",
    "df = pd.read_csv(f'{DATASET_NAME}_processed.csv')\n",
    "df_train, df_test = train_test_split(df, test_size=TRAIN_TEST_RATIO, random_state=42, stratify = df[TARGET_VARIABLE])\n",
    "\n",
    "# Initialise logfile path\n",
    "timestamp = int(time.time())\n",
    "logfile = f'log_{DATASET_NAME}_{MODEL_NAME}_{timestamp}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0rmYtc28ZXk4"
   },
   "outputs": [],
   "source": [
    "# Grab x and y from the respective dataframes and convert to numpy arrays.\n",
    "train_x = df_train.drop(columns=TARGET_VARIABLE).values\n",
    "train_y = df_train[TARGET_VARIABLE].values\n",
    "test_x = df_test.drop(columns=TARGET_VARIABLE).values\n",
    "test_y = df_test[TARGET_VARIABLE].values\n",
    "\n",
    "# Initialise scaler and use this to normalize the inputs.\n",
    "scaler = StandardScaler()\n",
    "train_x = scaler.fit_transform(train_x)\n",
    "test_x = scaler.transform(test_x)\n",
    "\n",
    "# Some misc variables for pategan \n",
    "data_columns = [col for col in df_train.columns if col != TARGET_VARIABLE]\n",
    "class_ratios = df_train[TARGET_VARIABLE].sort_values().groupby(df_train[TARGET_VARIABLE]).size().values/train_x.shape[0]\n",
    "input_dim = train_x.shape[1]\n",
    "z_dim = int(input_dim / 4 + 1) if input_dim % 4 == 0 else int(input_dim / 4)\n",
    "conditional = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vMGXxmhnmEpz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 Loss D real :  0.009998809366402354 Loss D fake :  0.010185346211993226 Loss G :  0.010272271634336137 Epsilon spent :  2.623153807427913\n",
      "Epoch : 2 Loss D real :  0.009980876158773903 Loss D fake :  0.010269886215483387 Loss G :  0.010298191653309585 Epsilon spent :  2.9437225218617797\n",
      "Epoch : 3 Loss D real :  0.009979570551506892 Loss D fake :  0.010317872401817045 Loss G :  0.010317506082561073 Epsilon spent :  3.224422132116374\n",
      "Epoch : 4 Loss D real :  0.009999396572235376 Loss D fake :  0.010336159857109239 Loss G :  0.010385298509679692 Epsilon spent :  3.339819054074313\n",
      "Epoch : 5 Loss D real :  0.009959406347372277 Loss D fake :  0.010422250987970857 Loss G :  0.010432118975295718 Epsilon spent :  3.455215976032252\n",
      "Epoch : 6 Loss D real :  0.010124009351318521 Loss D fake :  0.010453518492777609 Loss G :  0.010461647396901662 Epsilon spent :  3.5706128979901908\n",
      "Epoch : 7 Loss D real :  0.010083417815733528 Loss D fake :  0.010402372263996425 Loss G :  0.010467464204714075 Epsilon spent :  3.6860098199481297\n",
      "Epoch : 8 Loss D real :  0.009946204690642673 Loss D fake :  0.010463146724485884 Loss G :  0.010576134926631755 Epsilon spent :  3.8014067419060686\n",
      "Epoch : 9 Loss D real :  0.01032079486563059 Loss D fake :  0.010517466590584189 Loss G :  0.01053586910987888 Epsilon spent :  3.916803663864008\n",
      "Epoch : 10 Loss D real :  0.010146168912721666 Loss D fake :  0.010448161640828835 Loss G :  0.0105484694206609 Epsilon spent :  4.032200585821947\n",
      "Epoch : 11 Loss D real :  0.009882424838519553 Loss D fake :  0.010509203448949924 Loss G :  0.010535691182606388 Epsilon spent :  4.147597507779886\n",
      "Epoch : 12 Loss D real :  0.009638818601049159 Loss D fake :  0.01050514252359959 Loss G :  0.010521501503057758 Epsilon spent :  4.262994429737825\n",
      "Epoch : 13 Loss D real :  0.010088348483270022 Loss D fake :  0.010516468807495176 Loss G :  0.010536137533128261 Epsilon spent :  4.378391351695764\n",
      "Epoch : 14 Loss D real :  0.00997673225042293 Loss D fake :  0.010528118012827256 Loss G :  0.010532215345558999 Epsilon spent :  4.493788273653703\n",
      "Epoch : 15 Loss D real :  0.009860113433834208 Loss D fake :  0.01050048760209766 Loss G :  0.010557630093396648 Epsilon spent :  4.609185195611642\n",
      "Epoch : 16 Loss D real :  0.009707837101553473 Loss D fake :  0.010456357735761148 Loss G :  0.01057831163784512 Epsilon spent :  4.724582117569581\n",
      "Epoch : 17 Loss D real :  0.009934555907678791 Loss D fake :  0.010440763891685504 Loss G :  0.010563833304636842 Epsilon spent :  4.8399790395275195\n",
      "Epoch : 18 Loss D real :  0.009958373981465882 Loss D fake :  0.010518479856133134 Loss G :  0.010568776321341237 Epsilon spent :  4.9553759614854584\n",
      "Epoch : 19 Loss D real :  0.009814298346967983 Loss D fake :  0.010456511993439777 Loss G :  0.01062525608319854 Epsilon spent :  5.070772883443397\n",
      "Epoch : 20 Loss D real :  0.0101526155845306 Loss D fake :  0.010425467652650329 Loss G :  0.010582706003541543 Epsilon spent :  5.186169805401336\n",
      "Epoch : 21 Loss D real :  0.010320455466667636 Loss D fake :  0.010518965032410075 Loss G :  0.010597380667835027 Epsilon spent :  5.301566727359275\n",
      "Epoch : 22 Loss D real :  0.009950199480970461 Loss D fake :  0.010490603705943782 Loss G :  0.010596200054246748 Epsilon spent :  5.416963649317214\n",
      "Epoch : 23 Loss D real :  0.01001677031131769 Loss D fake :  0.01046008033267755 Loss G :  0.010487111642385104 Epsilon spent :  5.532360571275153\n",
      "Epoch : 24 Loss D real :  0.009897004769132172 Loss D fake :  0.01056886693587063 Loss G :  0.010468996800432891 Epsilon spent :  5.647757493233092\n",
      "Epoch : 25 Loss D real :  0.009825556762609357 Loss D fake :  0.010392546527619579 Loss G :  0.010506107660909796 Epsilon spent :  5.750413762592017\n",
      "Epoch : 26 Loss D real :  0.009672020187008103 Loss D fake :  0.010382112356807145 Loss G :  0.010484572906711648 Epsilon spent :  5.826924640229428\n",
      "Epoch : 27 Loss D real :  0.009730846110402643 Loss D fake :  0.010450927529921012 Loss G :  0.010463700337304222 Epsilon spent :  5.903435517866839\n",
      "Epoch : 28 Loss D real :  0.009600688369519656 Loss D fake :  0.010449816844885246 Loss G :  0.010607722989193992 Epsilon spent :  5.97994639550425\n",
      "Epoch : 29 Loss D real :  0.010360705001902679 Loss D fake :  0.010445826858317006 Loss G :  0.010550166878501719 Epsilon spent :  6.05645727314166\n",
      "Epoch : 30 Loss D real :  0.00994708698215619 Loss D fake :  0.010398252946292024 Loss G :  0.010473689303368236 Epsilon spent :  6.132968150779072\n",
      "Epoch : 31 Loss D real :  0.009928027928844644 Loss D fake :  0.01052610621105069 Loss G :  0.010513013108841665 Epsilon spent :  6.2094790284164825\n",
      "Epoch : 32 Loss D real :  0.010005495342037768 Loss D fake :  0.010479609270708503 Loss G :  0.010458477051310162 Epsilon spent :  6.285989906053894\n",
      "Epoch : 33 Loss D real :  0.010152744010219072 Loss D fake :  0.010363612825010667 Loss G :  0.010439937270250459 Epsilon spent :  6.362500783691305\n",
      "Epoch : 34 Loss D real :  0.010080406312918858 Loss D fake :  0.010340100716288428 Loss G :  0.010490854039000939 Epsilon spent :  6.439011661328715\n",
      "Epoch : 35 Loss D real :  0.010024841331048945 Loss D fake :  0.010334590314150835 Loss G :  0.010380507842172152 Epsilon spent :  6.515522538966127\n",
      "Epoch : 36 Loss D real :  0.009814204884571746 Loss D fake :  0.010385196725987023 Loss G :  0.010399176141811855 Epsilon spent :  6.592033416603538\n",
      "Epoch : 37 Loss D real :  0.0097819652483594 Loss D fake :  0.010326020880873268 Loss G :  0.010496707811631437 Epsilon spent :  6.668544294240949\n",
      "Epoch : 38 Loss D real :  0.010027512035180609 Loss D fake :  0.010372427999091209 Loss G :  0.010500887598617639 Epsilon spent :  6.745055171878359\n",
      "Epoch : 39 Loss D real :  0.009841021910489233 Loss D fake :  0.010339923554462298 Loss G :  0.010430816613699247 Epsilon spent :  6.82156604951577\n"
     ]
    }
   ],
   "source": [
    "# Initialise hyperparams and do TRAINING\n",
    "Hyperparams = collections.namedtuple(\n",
    "        'Hyperarams',\n",
    "        'batch_size micro_batch_size clamp_lower clamp_upper clip_coeff sigma class_ratios lr num_epochs')\n",
    "Hyperparams.__new__.__defaults__ = (None, None, None, None, None, None, None, None, None)\n",
    "\n",
    "model = dp_wgan.DP_WGAN(LEAKY, logfile, input_dim, z_dim, TARGET_EPSILON, TARGET_DELTA, conditional)\n",
    "model.train(train_x, train_y, Hyperparams(batch_size=BATCH_SIZE, micro_batch_size=MICRO_BATCH_SIZE,\n",
    "                                              clamp_lower=CLAMP_LOWER, clamp_upper=CLAMP_UPPER,\n",
    "                                              clip_coeff=CLIP_COEFF, sigma=SIGMA, class_ratios=class_ratios, lr=LEARNING_RATE, \n",
    "                                              num_epochs=NUM_EPOCHS), private=ENABLE_PRIVACY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate synthetic data using trained model, then save in CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "FoR8Lqbmhace"
   },
   "outputs": [],
   "source": [
    "# Helper functions for saving the synthetic data...\n",
    "def update_array(indexes):\n",
    "    b = np.zeros((indexes.size, indexes.max()+1))\n",
    "    b[np.arange(indexes.size), indexes] = 1\n",
    "    return b\n",
    "\n",
    "def save_marketing():\n",
    "    # Some fancy indexing to get the actual synthetic data..\n",
    "    accepted = np.argmax(syn_save[:,16:21], axis=1)\n",
    "    education = np.argmax(syn_save[:, 22:27], axis=1)\n",
    "    marital = np.argmax(syn_save[:, 27:34], axis=1)\n",
    "    country = np.argmax(syn_save[:, 34:], axis=1)\n",
    "\n",
    "    syn_save[:,16:21] = update_array(accepted)\n",
    "    syn_save[:, 22:27] = update_array(education)\n",
    "    syn_save[:, 27:34] = update_array(marital)\n",
    "    syn_save[:, 34:] = update_array(country)\n",
    "\n",
    "    df1 = pd.DataFrame(syn_save, columns = df.columns.drop(TARGET_VARIABLE))\n",
    "    df2 = pd.DataFrame(syn_y, columns = [TARGET_VARIABLE])\n",
    "    df_save = pd.concat([df1,df2], axis =1)\n",
    "    df_save.to_csv(f'synthetic_{MODEL_NAME}_{DATASET_NAME}.csv')\n",
    "\n",
    "def save_churn():\n",
    "    geography = np.argmax(syn_save[:,8:11], axis=1)\n",
    "    gender = np.argmax(syn_save[:,11:], axis=1)\n",
    "    \n",
    "    syn_save[:,8:11] = update_array(geography)\n",
    "    syn_save[:, 11:] = update_array(gender)\n",
    "    \n",
    "    df1 = pd.DataFrame(syn_save, columns = df.columns.drop(TARGET_VARIABLE))\n",
    "    df2 = pd.DataFrame(syn_y, columns = [TARGET_VARIABLE])\n",
    "    df_save = pd.concat([df1,df2], axis =1)\n",
    "    df_save.to_csv(f'synthetic_{MODEL_NAME}_{DATASET_NAME}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to generate data and save them.\n",
    "syn_data = model.generate(train_x.shape[0], class_ratios)\n",
    "syn_x, syn_y = syn_data[:, :-1], syn_data[:, -1]\n",
    "\n",
    "# Make a copy for saving\n",
    "syn_save = scaler.inverse_transform(syn_x)\n",
    "\n",
    "# Save data to csv using functions\n",
    "if TARGET_VARIABLE == 'churn': save_churn()\n",
    "else: save_marketing()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do classification using Neural Networks and look at ROC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2346,
     "status": "ok",
     "timestamp": 1622894670836,
     "user": {
      "displayName": "Tee Yee Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgKFrQtw-OyvubwOe5xvtv9JOTRkaAVybEHvmq9ag=s64",
      "userId": "07106232567566268752"
     },
     "user_tz": -480
    },
    "id": "QRtEB0Orm7ah",
    "outputId": "fcd7a205-093c-4447-c924-0cd4cd27e463"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38112961399739087\n",
      "0.6227436823104693\n"
     ]
    }
   ],
   "source": [
    "# Train using Synthetic data, using simple neural network.\n",
    "mlp = MLPClassifier((32,8), max_iter=1000, random_state=42)\n",
    "mlp.fit(syn_x, syn_y)\n",
    "pred_y = mlp.predict(test_x_syn)\n",
    "\n",
    "print(roc_auc_score(test_y, pred_y))\n",
    "print(mlp.score(test_x_syn,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3867,
     "status": "ok",
     "timestamp": 1622894688257,
     "user": {
      "displayName": "Tee Yee Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgKFrQtw-OyvubwOe5xvtv9JOTRkaAVybEHvmq9ag=s64",
      "userId": "07106232567566268752"
     },
     "user_tz": -480
    },
    "id": "nB_uyefotvDF",
    "outputId": "d4bb6ab0-3b38-4031-ba65-4d29183f4c7a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8664259927797834\n",
      "0.7328677768398435\n"
     ]
    }
   ],
   "source": [
    "# Train using REAL data, using simple neural network.\n",
    "mlp = MLPClassifier((32,8), max_iter=1000, random_state=42)\n",
    "mlp.fit(train_x, train_y)\n",
    "print(mlp.score(test_x, test_y))\n",
    "\n",
    "pred_y = mlp.predict(test_x)\n",
    "print(roc_auc_score(test_y,pred_y))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dpwgan-fullscript.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:pategan]",
   "language": "python",
   "name": "conda-env-pategan-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
