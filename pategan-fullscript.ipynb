{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ijsbqc7Ipl2"
   },
   "source": [
    "## Full PATEGAN rewrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 468,
     "status": "ok",
     "timestamp": 1622889465948,
     "user": {
      "displayName": "Tee Yee Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgKFrQtw-OyvubwOe5xvtv9JOTRkaAVybEHvmq9ag=s64",
      "userId": "07106232567566268752"
     },
     "user_tz": -480
    },
    "id": "QifCHxjTI5Ql"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from models import dp_wgan, pate_gan, ron_gauss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 288,
     "status": "ok",
     "timestamp": 1622889479099,
     "user": {
      "displayName": "Tee Yee Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgKFrQtw-OyvubwOe5xvtv9JOTRkaAVybEHvmq9ag=s64",
      "userId": "07106232567566268752"
     },
     "user_tz": -480
    },
    "id": "KcDKmo_aisiA"
   },
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "MODEL_NAME = 'PATEGAN' # Don't change this\n",
    "DATASET_NAME = 'churn' # Choose either 'churn' or 'marketing' exactly!\n",
    "TARGET_VARIABLE = 'Exited' # either 'Exited' or 'Response'\n",
    "TRAIN_TEST_RATIO = 0.25\n",
    "\n",
    "# These seem to be good to tune from what I can tell from the github.\n",
    "NUM_TEACHERS = 20\n",
    "TARGET_EPSILON = 1\n",
    "TARGET_DELTA = 1e-4\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# Following defaults in the toolbox. Might not be crucial to tune these\n",
    "BATCH_SIZE = 64\n",
    "TEACHER_ITER = 5\n",
    "STUDENT_ITER = 5\n",
    "NUM_MOMENTS= 100\n",
    "LAP_SCALE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1622889465949,
     "user": {
      "displayName": "Tee Yee Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgKFrQtw-OyvubwOe5xvtv9JOTRkaAVybEHvmq9ag=s64",
      "userId": "07106232567566268752"
     },
     "user_tz": -480
    },
    "id": "6LXKHydGhhEF"
   },
   "outputs": [],
   "source": [
    "# Read in data and do train test split\n",
    "df = pd.read_csv(f'{DATASET_NAME}_processed.csv')\n",
    "df_train, df_test = train_test_split(df, test_size=TRAIN_TEST_RATIO, random_state=42, stratify = df[TARGET_VARIABLE])\n",
    "\n",
    "# Initialise logfile path\n",
    "timestamp = int(time.time())\n",
    "logfile = f'log_{DATASET_NAME}_{MODEL_NAME}_{timestamp}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1622889465949,
     "user": {
      "displayName": "Tee Yee Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgKFrQtw-OyvubwOe5xvtv9JOTRkaAVybEHvmq9ag=s64",
      "userId": "07106232567566268752"
     },
     "user_tz": -480
    },
    "id": "0rmYtc28ZXk4"
   },
   "outputs": [],
   "source": [
    "# Grab x and y from the respective dataframes and convert to numpy arrays.\n",
    "train_x = df_train.drop(columns=TARGET_VARIABLE).values\n",
    "train_y = df_train[TARGET_VARIABLE].values\n",
    "test_x = df_test.drop(columns=TARGET_VARIABLE).values\n",
    "test_y = df_test[TARGET_VARIABLE].values\n",
    "\n",
    "# Initialise scaler and use this to normalize the inputs.\n",
    "scaler = StandardScaler()\n",
    "train_x = scaler.fit_transform(train_x)\n",
    "test_x = scaler.transform(test_x)\n",
    "\n",
    "# Some misc variables for pategan \n",
    "data_columns = [col for col in df_train.columns if col != TARGET_VARIABLE]\n",
    "class_ratios = df_train[TARGET_VARIABLE].sort_values().groupby(df_train[TARGET_VARIABLE]).size().values/train_x.shape[0]\n",
    "input_dim = train_x.shape[1]\n",
    "z_dim = int(input_dim / 4 + 1) if input_dim % 4 == 0 else int(input_dim / 4)\n",
    "conditional = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vMGXxmhnmEpz",
    "outputId": "369c8848-a002-45d0-b434-f5cd5f837c32",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1587428207430/work/aten/src/ATen/native/TensorFactories.cpp:361: UserWarning: Deprecation warning: In a future PyTorch release torch.full will no longer return tensors of floating dtype by default. Instead, a bool fill_value will return a tensor of torch.bool dtype, and an integral fill_value will return a tensor of torch.long dtype. Set the optional `dtype` or `out` arguments to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step :  0 Loss SD :  0.695424205706462 Loss G :  0.695707886430293 Epsilon :  0.09274980371976183\n",
      "Step :  100 Loss SD :  0.6907101654998429 Loss G :  0.6755311964472788 Epsilon :  0.1573898037197612\n",
      "Step :  200 Loss SD :  0.6862505276391577 Loss G :  0.6678520957412795 Epsilon :  0.21898734555266197\n",
      "Step :  300 Loss SD :  0.700383739318257 Loss G :  0.6631237147055132 Epsilon :  0.2683311937967585\n",
      "Step :  400 Loss SD :  0.6928733267284874 Loss G :  0.6786046061924935 Epsilon :  0.31005607286626513\n",
      "Step :  500 Loss SD :  0.6909147271069437 Loss G :  0.6763123526341528 Epsilon :  0.34691385874029596\n",
      "Step :  600 Loss SD :  0.7034042802741669 Loss G :  0.6586478355137176 Epsilon :  0.38028613004031625\n",
      "Step :  700 Loss SD :  0.6843089905500973 Loss G :  0.6614123573812569 Epsilon :  0.41104863048834817\n",
      "Step :  800 Loss SD :  0.6978987938223963 Loss G :  0.6556273230811728 Epsilon :  0.4397290183803887\n",
      "Step :  900 Loss SD :  0.7020671049354503 Loss G :  0.6595154856855716 Epsilon :  0.46668090929940115\n",
      "Step :  1000 Loss SD :  0.6952809768882773 Loss G :  0.6715200973123869 Epsilon :  0.492226978209883\n",
      "Step :  1100 Loss SD :  0.7001411685708155 Loss G :  0.668986163886368 Epsilon :  0.5165595881104224\n",
      "Step :  1200 Loss SD :  0.6955887279063417 Loss G :  0.6756680948654403 Epsilon :  0.5398629820564878\n",
      "Step :  1300 Loss SD :  0.6861870387170403 Loss G :  0.6732846668489341 Epsilon :  0.562198823393227\n",
      "Step :  1400 Loss SD :  0.7010596339521493 Loss G :  0.6601634034517807 Epsilon :  0.583714336624279\n",
      "Step :  1500 Loss SD :  0.6903845327302398 Loss G :  0.6752867366144215 Epsilon :  0.6045125539346552\n",
      "Step :  1600 Loss SD :  0.6890829386810686 Loss G :  0.6566267832291973 Epsilon :  0.6246497457325176\n",
      "Step :  1700 Loss SD :  0.688166210948513 Loss G :  0.6614330293715285 Epsilon :  0.6441899438612608\n",
      "Step :  1800 Loss SD :  0.6962585522769416 Loss G :  0.6714652427476437 Epsilon :  0.6632063275705914\n",
      "Step :  1900 Loss SD :  0.688718879288637 Loss G :  0.6766112066839184 Epsilon :  0.6817663275705922\n",
      "Step :  2000 Loss SD :  0.6904988779088462 Loss G :  0.6715889822706893 Epsilon :  0.6997029174805695\n",
      "Step :  2100 Loss SD :  0.6918206829971028 Loss G :  0.6750739827094049 Epsilon :  0.7172966604606683\n",
      "Step :  2200 Loss SD :  0.694423579703974 Loss G :  0.6662801655003333 Epsilon :  0.7345766604606596\n",
      "Step :  2300 Loss SD :  0.6928187734752685 Loss G :  0.6656308458321467 Epsilon :  0.7513000148790926\n",
      "Step :  2400 Loss SD :  0.6834289401196572 Loss G :  0.6820152540852418 Epsilon :  0.7679241821656911\n",
      "Step :  2500 Loss SD :  0.6978202479040788 Loss G :  0.6872912870667605 Epsilon :  0.7839241821657071\n",
      "Step :  2600 Loss SD :  0.7006508044374327 Loss G :  0.6957100104788649 Epsilon :  0.7999241821657231\n",
      "Step :  2700 Loss SD :  0.6895751970646097 Loss G :  0.6780043596610861 Epsilon :  0.8153231813902458\n",
      "Step :  2800 Loss SD :  0.6921118158433668 Loss G :  0.673611564490031 Epsilon :  0.8306831813902318\n",
      "Step :  2900 Loss SD :  0.691667856093428 Loss G :  0.6909226456587827 Epsilon :  0.8456790350899006\n",
      "Step :  3000 Loss SD :  0.6971458042911225 Loss G :  0.6904068491888655 Epsilon :  0.860399035089904\n",
      "Step :  3100 Loss SD :  0.6975577121255447 Loss G :  0.6842929304985359 Epsilon :  0.8751190350899075\n",
      "Step :  3200 Loss SD :  0.6924941794088648 Loss G :  0.6924466862343761 Epsilon :  0.8892884367606044\n"
     ]
    }
   ],
   "source": [
    "# Initialise hyperparams and do TRAINING\n",
    "Hyperparams = collections.namedtuple(\n",
    "        'Hyperarams',\n",
    "        'batch_size num_teacher_iters num_student_iters num_moments lap_scale class_ratios lr')\n",
    "Hyperparams.__new__.__defaults__ = (None, None, None, None, None, None, None)\n",
    "\n",
    "model = pate_gan.PATE_GAN(logfile, input_dim, z_dim, NUM_TEACHERS, TARGET_EPSILON, TARGET_DELTA, conditional)\n",
    "model.train(train_x, train_y, Hyperparams(batch_size=BATCH_SIZE, num_teacher_iters=TEACHER_ITER,\n",
    "                                              num_student_iters=STUDENT_ITER, num_moments=NUM_MOMENTS,\n",
    "                                              lap_scale=LAP_SCALE, class_ratios=class_ratios, lr=LEARNING_RATE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate synthetic data using trained model, then save in CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for saving the synthetic data...\n",
    "def update_array(indexes):\n",
    "    b = np.zeros((indexes.size, indexes.max()+1))\n",
    "    b[np.arange(indexes.size), indexes] = 1\n",
    "    return b\n",
    "\n",
    "def save_marketing():\n",
    "    # Some fancy indexing to get the actual synthetic data..\n",
    "    accepted = np.argmax(syn_save[:,16:21], axis=1)\n",
    "    education = np.argmax(syn_save[:, 22:27], axis=1)\n",
    "    marital = np.argmax(syn_save[:, 27:34], axis=1)\n",
    "    country = np.argmax(syn_save[:, 34:], axis=1)\n",
    "\n",
    "    syn_save[:,16:21] = update_array(accepted)\n",
    "    syn_save[:, 22:27] = update_array(education)\n",
    "    syn_save[:, 27:34] = update_array(marital)\n",
    "    syn_save[:, 34:] = update_array(country)\n",
    "\n",
    "    df1 = pd.DataFrame(syn_save, columns = df.columns.drop(TARGET_VARIABLE))\n",
    "    df2 = pd.DataFrame(syn_y, columns = [TARGET_VARIABLE])\n",
    "    df_save = pd.concat([df1,df2], axis =1)\n",
    "    df_save.to_csv(f'synthetic_{MODEL_NAME}_{DATASET_NAME}.csv')\n",
    "\n",
    "def save_churn():\n",
    "    geography = np.argmax(syn_save[:,8:11], axis=1)\n",
    "    gender = np.argmax(syn_save[:,11:], axis=1)\n",
    "    \n",
    "    syn_save[:,8:11] = update_array(geography)\n",
    "    syn_save[:, 11:] = update_array(gender)\n",
    "    \n",
    "    df1 = pd.DataFrame(syn_save, columns = df.columns.drop(TARGET_VARIABLE))\n",
    "    df2 = pd.DataFrame(syn_y, columns = [TARGET_VARIABLE])\n",
    "    df_save = pd.concat([df1,df2], axis =1)\n",
    "    df_save.to_csv(f'synthetic_{MODEL_NAME}_{DATASET_NAME}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to generate data and save them.\n",
    "syn_data = model.generate(train_x.shape[0], class_ratios)\n",
    "syn_x, syn_y = syn_data[:, :-1], syn_data[:, -1]\n",
    "\n",
    "# Make a copy for saving\n",
    "syn_save = scaler.inverse_transform(syn_x)\n",
    "\n",
    "# Save data to csv using functions\n",
    "if TARGET_VARIABLE == 'churn': save_churn()\n",
    "else: save_marketing()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do classification using Neural Networks and look at ROC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 293,
     "status": "ok",
     "timestamp": 1622888794774,
     "user": {
      "displayName": "Tee Yee Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgKFrQtw-OyvubwOe5xvtv9JOTRkaAVybEHvmq9ag=s64",
      "userId": "07106232567566268752"
     },
     "user_tz": -480
    },
    "id": "QRtEB0Orm7ah"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC Score 0.5844908295602793\n",
      "Accuracy 0.723826714801444\n"
     ]
    }
   ],
   "source": [
    "# Train using Synthetic data, using simple neural network.\n",
    "mlp = MLPClassifier((32,8), max_iter=1000, random_state=42)\n",
    "mlp.fit(syn_x, syn_y)\n",
    "pred_y = mlp.predict(test_x)\n",
    "\n",
    "print('ROC Score', roc_auc_score(test_y, pred_y))\n",
    "print('Accuracy', mlp.score(test_x,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8818,
     "status": "ok",
     "timestamp": 1622888926414,
     "user": {
      "displayName": "Tee Yee Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgKFrQtw-OyvubwOe5xvtv9JOTRkaAVybEHvmq9ag=s64",
      "userId": "07106232567566268752"
     },
     "user_tz": -480
    },
    "id": "nB_uyefotvDF",
    "outputId": "6d0b252e-3584-44fb-b7e2-978045f41be5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC Score 0.7328677768398435\n",
      "Accuracy 0.8664259927797834\n"
     ]
    }
   ],
   "source": [
    "# Train using REAL data, using simple neural network.\n",
    "mlp = MLPClassifier((32,8), max_iter=1000, random_state=42)\n",
    "mlp.fit(train_x, train_y)\n",
    "pred_y = mlp.predict(test_x)\n",
    "\n",
    "print('ROC Score', roc_auc_score(test_y, pred_y))\n",
    "print('Accuracy', mlp.score(test_x,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pategan-fullscript.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:pategan]",
   "language": "python",
   "name": "conda-env-pategan-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
